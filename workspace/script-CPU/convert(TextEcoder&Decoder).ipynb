{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.schedulers import PNDMScheduler\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from torch.onnx import export\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from diffusers import DiffusionPipeline\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.11\")\n",
    "\n",
    "def onnx_export(\n",
    "    \n",
    "    model,\n",
    "    model_args: tuple,\n",
    "    output_path: Path,\n",
    "    ordered_input_names,\n",
    "    output_names,\n",
    "    dynamic_axes,\n",
    "    opset,\n",
    "    use_external_data_format=False,\n",
    "):\n",
    "    print('ONNX export StartðŸš—')\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # PyTorch deprecated the `enable_onnx_checker` and `use_external_data_format` arguments in v1.11,\n",
    "    # so we check the torch version for backwards compatibility\n",
    "    if is_torch_less_than_1_11:\n",
    "        export(\n",
    "            model,\n",
    "            model_args,\n",
    "            f=output_path.as_posix(),\n",
    "            input_names=ordered_input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "            do_constant_folding=True,\n",
    "            use_external_data_format=use_external_data_format,\n",
    "            enable_onnx_checker=True,\n",
    "            opset_version=opset,\n",
    "        )\n",
    "    else:\n",
    "        export(\n",
    "            model,\n",
    "            model_args,\n",
    "            f=output_path.as_posix(),\n",
    "            input_names=ordered_input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "            do_constant_folding=True,\n",
    "            opset_version=opset,\n",
    "        )\n",
    "    print('ONNX export FinishðŸ·')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, tokenizer, textencoder, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = textencoder.to(device = device)\n",
    "        self.device = device\n",
    "       \n",
    "    def forward(self, text_ids):\n",
    "        # uncond-input ì¤€ë¹„\n",
    "        uncond_input = self.tokenizer(\n",
    "            [\"\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=pipeline.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "        # ì¸ì½”ë”© \n",
    "        textembed               = self.text_encoder(text_ids.to(device=self.device, dtype=torch.int32)).last_hidden_state\n",
    "        negative_prompt_embeds  = self.text_encoder(uncond_input.to(device=self.device, dtype=torch.int32)).last_hidden_state\n",
    "        \n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, textembed])\n",
    "        return prompt_embeds\n",
    "        \n",
    "te = TextEmbedding(pipeline.tokenizer, pipeline.text_encoder, 'cuda')\n",
    "text_input = pipeline.tokenizer(\n",
    "    [\"A sample prompt\"],\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipeline.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    ).input_ids.to(device=device, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    }
   ],
   "source": [
    "onnx_export(\n",
    "    TextEmbedding(pipeline.tokenizer, pipeline.text_encoder, 'cpu'),\n",
    "    model_args=(\n",
    "        text_input\n",
    "    ),\n",
    "    output_path = Path('./onnx-models/TextEmbedding/model.onnx'),\n",
    "    ordered_input_names=[\"prompt\"],\n",
    "    output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "    dynamic_axes={\n",
    "        \"prompt\": {0: \"batch\", 1: \"sequence\"},\n",
    "    },\n",
    "    opset=14,\n",
    "    use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### VAE Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vae_decoder, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.vae_decoder = vae_decoder\n",
    "        self.vae_decoder = vae_decoder.to(device = device)\n",
    "        self.device = device\n",
    "       \n",
    "    def forward(self, latent_sample):\n",
    "        latent_sample = 1 / 0.18215 * latent_sample\n",
    "        # í†µê³¼\n",
    "        image = self.vae_decoder(latent_sample)['sample']       # [1, 3, 512, 512]\n",
    "        image = torch.clip(image / 2 + 0.5, 0, 1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    }
   ],
   "source": [
    "vae_decoder = pipeline.vae\n",
    "vae_decoder.forward = pipeline.vae.decode\n",
    "\n",
    "onnx_export(\n",
    "    Decoder(vae_decoder, 'cpu'),\n",
    "    model_args=(\n",
    "        torch.randn(1, 4, 64, 64).to(device=device, dtype=dtype),\n",
    "    ),\n",
    "    output_path = Path('./onnx-models/Decoder/model.onnx'),\n",
    "    ordered_input_names=[\"latent_sample\"],\n",
    "    output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "    dynamic_axes={\n",
    "        \"latent_sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n",
    "    },\n",
    "    opset=14,\n",
    "    use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
